[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a data scientist working in biotech. I’m hoping to post more regularly and share some of the cool things i’ve learned over the years."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "I’m hoping to post every few days structured mini-projects with the hopes that can prove helpful to analysis of any skill level. The bulk of what you’ll see is Pandas, Matplotlib, and skikit-learn code. Hope you get to learn a thing or two!"
  },
  {
    "objectID": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html",
    "href": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html",
    "title": "Data Analysis with Pandas, Plotly and Seaborn - Part 1",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings('ignore')\nimport plotly.io as pio\nimport matplotlib.pyplot as plt\nimport plotly.express as px \nimport seaborn as sns\nplt.style.use('ggplot')\npio.templates.default = \"ggplot2\""
  },
  {
    "objectID": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#intro",
    "href": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#intro",
    "title": "Data Analysis with Pandas, Plotly and Seaborn - Part 1",
    "section": "Intro",
    "text": "Intro\n\nWhat is Pandas?\n\npandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n\n\n\nWhat is Plotly?\nAn interactive ploting library for Python\n\n\nWhat is Seaborn?\nA data visualization tool (usually static) built on top of matplotlib\n\n\nWho is this for?\n\nNovice Analysts\nMed-Advanced Analysis Transitioning to Python\nSomeone who needs a refreser on the basics\n\n\n\nData Structures\n\n\nA DataFrame is made of Series structures\n\nA Series has a Row Names (index) and Column Names (columns) containing an array of the same value type (integer, float, string, etc..)"
  },
  {
    "objectID": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#data-munging",
    "href": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#data-munging",
    "title": "Data Analysis with Pandas, Plotly and Seaborn - Part 1",
    "section": "Data Munging",
    "text": "Data Munging\n\nReading In Data\nWe’re going to explore a dataset that contains a variety of wines with written reviews, rating, price among other features.\n\nimport pandas as pd\ndf = pd.read_csv(\"data/winemag-data_first150k.csv\", index_col=0)\ndf.head()\n\n\n\n\n\n\n\n\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\nvariety\nwinery\n\n\n\n\n0\nUS\nThis tremendous 100% varietal wine hails from ...\nMartha's Vineyard\n96\n235.0\nCalifornia\nNapa Valley\nNapa\nCabernet Sauvignon\nHeitz\n\n\n1\nSpain\nRipe aromas of fig, blackberry and cassis are ...\nCarodorum Selección Especial Reserva\n96\n110.0\nNorthern Spain\nToro\nNaN\nTinta de Toro\nBodega Carmen Rodríguez\n\n\n2\nUS\nMac Watson honors the memory of a wine once ma...\nSpecial Selected Late Harvest\n96\n90.0\nCalifornia\nKnights Valley\nSonoma\nSauvignon Blanc\nMacauley\n\n\n3\nUS\nThis spent 20 months in 30% new French oak, an...\nReserve\n96\n65.0\nOregon\nWillamette Valley\nWillamette Valley\nPinot Noir\nPonzi\n\n\n4\nFrance\nThis is the top wine from La Bégude, named aft...\nLa Brûlade\n95\n66.0\nProvence\nBandol\nNaN\nProvence red blend\nDomaine de la Bégude\n\n\n\n\n\n\n\n\n\nData Types (dtypes)\n\nEach column must have a single type\n\nObject (strings, or any python object (like a list)\nFloating (float)\nInteger (int)\n\n\n\ndf.dtypes\n\ncountry         object\ndescription     object\ndesignation     object\npoints           int64\nprice          float64\nprovince        object\nregion_1        object\nregion_2        object\nvariety         object\nwinery          object\ndtype: object\n\n\n\nNote: Missing values are treated with a special NaN value, which is a float. This will convert your int dtypes to float if Pandas finds missing values in the column."
  },
  {
    "objectID": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#eda",
    "href": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#eda",
    "title": "Data Analysis with Pandas, Plotly and Seaborn - Part 1",
    "section": "EDA",
    "text": "EDA\nExploring the data can get complicated. Pandas provides methods for exploration and processing tabular data.\n\n# Return the first 10 columns names\ndf.columns[:10]\n\nIndex(['country', 'description', 'designation', 'points', 'price', 'province',\n       'region_1', 'region_2', 'variety', 'winery'],\n      dtype='object')\n\n\n\n# Return the first 10 row names\ndf.index[:10]\n\nInt64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n\n\n\ndf.head(n=2)\n\n\n\n\n\n\n\n\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\nvariety\nwinery\n\n\n\n\n0\nUS\nThis tremendous 100% varietal wine hails from ...\nMartha's Vineyard\n96\n235.0\nCalifornia\nNapa Valley\nNapa\nCabernet Sauvignon\nHeitz\n\n\n1\nSpain\nRipe aromas of fig, blackberry and cassis are ...\nCarodorum Selección Especial Reserva\n96\n110.0\nNorthern Spain\nToro\nNaN\nTinta de Toro\nBodega Carmen Rodríguez\n\n\n\n\n\n\n\n\ndf.tail(n=2)\n\n\n\n\n\n\n\n\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\nvariety\nwinery\n\n\n\n\n150928\nFrance\nA perfect salmon shade, with scents of peaches...\nGrand Brut Rosé\n90\n52.0\nChampagne\nChampagne\nNaN\nChampagne Blend\nGosset\n\n\n150929\nItaly\nMore Pinot Grigios should taste like this. A r...\nNaN\n90\n15.0\nNortheastern Italy\nAlto Adige\nNaN\nPinot Grigio\nAlois Lageder\n\n\n\n\n\n\n\n\nColumn Selection\n\ndf['country'].head()\n\n0        US\n1     Spain\n2        US\n3        US\n4    France\nName: country, dtype: object\n\n\n\ndf[['country', 'points']].head() # Use lists for selecting multiple columns\n\n\n\n\n\n\n\n\ncountry\npoints\n\n\n\n\n0\nUS\n96\n\n\n1\nSpain\n96\n\n\n2\nUS\n96\n\n\n3\nUS\n96\n\n\n4\nFrance\n95\n\n\n\n\n\n\n\n\n\nFinding Data\nLoc Approach uses the indexes to locate the data you’re looking for.\n\ndf.loc[150929, 'country'] # Returns a string \n\n'Italy'\n\n\n\ndf.loc[150928:150930, 'country'] # Returns a series (of strings)\n\n150928    France\n150929     Italy\nName: country, dtype: object\n\n\n\ndf.loc[[150928, 150929], ['country', 'price']] # Returns a dataframe\n\n\n\n\n\n\n\n\ncountry\nprice\n\n\n\n\n150928\nFrance\n52.0\n\n\n150929\nItaly\n15.0\n\n\n\n\n\n\n\nIn many cases, we won’t know the index values that we’re looking for.\nWe’re often more interested in searching for specific values stored in one (or more) of the columns.\nHow would we do this?\n\n\nBoolean Masking\nEx: I want to know which rows in the country column equal Italy\nStep 1: Access the country Series object\nStep 2: Set an equality condition\n\ndf['country'] == 'Italy'\n\n0         False\n1         False\n2         False\n3         False\n4         False\n          ...  \n150925     True\n150926    False\n150927     True\n150928    False\n150929     True\nName: country, Length: 150930, dtype: bool\n\n\nThe result is a boolean series telling us which indexes equal “Italy”\nWe can transfer this operation into our .loc operation\n\nitalian_df = df.loc[(df['country'] == 'Italy')]\nitalian_df.head()\n# This contains a new copy of our dataframe with only Italy countries\n\n\n\n\n\n\n\n\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\nvariety\nwinery\n\n\n\n\n10\nItaly\nElegance, complexity and structure come togeth...\nRonco della Chiesa\n95\n80.0\nNortheastern Italy\nCollio\nNaN\nFriulano\nBorgo del Tiglio\n\n\n32\nItaly\nUnderbrush, scorched earth, menthol and plum s...\nVigna Piaggia\n90\nNaN\nTuscany\nBrunello di Montalcino\nNaN\nSangiovese\nAbbadia Ardenga\n\n\n35\nItaly\nForest floor, tilled soil, mature berry and a ...\nRiserva\n90\n135.0\nTuscany\nBrunello di Montalcino\nNaN\nSangiovese\nCarillon\n\n\n37\nItaly\nAromas of forest floor, violet, red berry and ...\nNaN\n90\n29.0\nTuscany\nVino Nobile di Montepulciano\nNaN\nSangiovese\nAvignonesi\n\n\n38\nItaly\nThis has a charming nose that boasts rose, vio...\nNaN\n90\n23.0\nTuscany\nChianti Classico\nNaN\nSangiovese\nCasina di Cornia\n\n\n\n\n\n\n\nThis is the most common way to search for data in Pandas.\nWe can combine boolean vectors using AND & and OR |\n\nit_or_fr = (df['country'] == 'Italy') | (df['country'] == 'France')\n\n\n# Show me wines from Italy OR France at or above the 95 percentile of the entire dataset\nexpensive_wine = df['price'] &gt; df['price'].quantile(.95)\n\n\n# Get me the rows where the row name is true for country italy or france and has expensive wine\ndf.loc[(it_or_fr) & (expensive_wine), ['country', 'variety', 'price', 'points']]\n\n\n\n\n\n\n\n\ncountry\nvariety\nprice\npoints\n\n\n\n\n13\nFrance\nTannat\n90.0\n95\n\n\n18\nFrance\nMalbec\n290.0\n95\n\n\n35\nItaly\nSangiovese\n135.0\n90\n\n\n46\nItaly\nSangiovese\n90.0\n90\n\n\n50\nItaly\nSangiovese\n100.0\n90\n\n\n...\n...\n...\n...\n...\n\n\n149186\nFrance\nBordeaux-style Red Blend\n85.0\n94\n\n\n149187\nFrance\nBordeaux-style Red Blend\n260.0\n94\n\n\n149299\nFrance\nBordeaux-style Red Blend\n100.0\n93\n\n\n149344\nFrance\nAlsace white blend\n95.0\n86\n\n\n149471\nFrance\nBordeaux-style Red Blend\n115.0\n91\n\n\n\n\n3025 rows × 4 columns\n\n\n\niloc uses integer locations instead\n\ndf.iloc[150929, 0] # English: Get the 150929th row and the 0th column \n\n'Italy'\n\n\n\n\nOperations on Data\n\n# Take the mean of the price column and round to two decimals\n# \ndf['points'].mean().round(2)\n\n87.89\n\n\n\n# Take the list of aggregations mean and median. Returns a series. \ndf['points'].agg(['mean', 'median'])\n\nmean      87.888418\nmedian    88.000000\nName: points, dtype: float64\n\n\n\ndf.loc[:,'points'].describe()\n\ncount    150930.000000\nmean         87.888418\nstd           3.222392\nmin          80.000000\n25%          86.000000\n50%          88.000000\n75%          90.000000\nmax         100.000000\nName: points, dtype: float64\n\n\nOr combine series objects and create new columns\n\n# Assign this new feature to a column.\ndf['point_price_ratio'] = (df['points'] / df['price'])\ndf[['points', 'price', 'point_price_ratio']].head()\n\n\n\n\n\n\n\n\npoints\nprice\npoint_price_ratio\n\n\n\n\n0\n96\n235.0\n0.408511\n\n\n1\n96\n110.0\n0.872727\n\n\n2\n96\n90.0\n1.066667\n\n\n3\n96\n65.0\n1.476923\n\n\n4\n95\n66.0\n1.439394\n\n\n\n\n\n\n\n\n\nMissing Values\n\n# Find missing values in price\ndf.loc[df['price'].isna()].head()\n\n\n\n\n\n\n\n\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\nvariety\nwinery\npoint_price_ratio\n\n\n\n\n32\nItaly\nUnderbrush, scorched earth, menthol and plum s...\nVigna Piaggia\n90\nNaN\nTuscany\nBrunello di Montalcino\nNaN\nSangiovese\nAbbadia Ardenga\nNaN\n\n\n56\nFrance\nDelicious while also young and textured, this ...\nLe Pavé\n90\nNaN\nLoire Valley\nSancerre\nNaN\nSauvignon Blanc\nDomaine Vacheron\nNaN\n\n\n72\nItaly\nThis offers aromas of red rose, wild berry, da...\nBussia Riserva\n91\nNaN\nPiedmont\nBarolo\nNaN\nNebbiolo\nSilvano Bolmida\nNaN\n\n\n82\nItaly\nBerry, baking spice, dried iris, mint and a hi...\nPalliano Riserva\n91\nNaN\nPiedmont\nRoero\nNaN\nNebbiolo\nCeste\nNaN\n\n\n116\nSpain\nAromas of brandied cherry and crème de cassis ...\nDulce Tinto\n86\nNaN\nLevante\nJumilla\nNaN\nMonastrell\nCasa de la Ermita\nNaN\n\n\n\n\n\n\n\nMissing values can be dropped or filled in (or left alone)\n\n# Drop any rows containing missing values\ndf.dropna().head()\n\n\n\n\n\n\n\n\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\nvariety\nwinery\npoint_price_ratio\n\n\n\n\n0\nUS\nThis tremendous 100% varietal wine hails from ...\nMartha's Vineyard\n96\n235.0\nCalifornia\nNapa Valley\nNapa\nCabernet Sauvignon\nHeitz\n0.408511\n\n\n2\nUS\nMac Watson honors the memory of a wine once ma...\nSpecial Selected Late Harvest\n96\n90.0\nCalifornia\nKnights Valley\nSonoma\nSauvignon Blanc\nMacauley\n1.066667\n\n\n3\nUS\nThis spent 20 months in 30% new French oak, an...\nReserve\n96\n65.0\nOregon\nWillamette Valley\nWillamette Valley\nPinot Noir\nPonzi\n1.476923\n\n\n8\nUS\nThis re-named vineyard was formerly bottled as...\nSilice\n95\n65.0\nOregon\nChehalem Mountains\nWillamette Valley\nPinot Noir\nBergström\n1.461538\n\n\n9\nUS\nThe producer sources from two blocks of the vi...\nGap's Crown Vineyard\n95\n60.0\nCalifornia\nSonoma Coast\nSonoma\nPinot Noir\nBlue Farm\n1.583333\n\n\n\n\n\n\n\n\n# Drop any rows where price or points are missing\ndf.dropna(subset=['price', 'points']).head()\n\n\n\n\n\n\n\n\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\nvariety\nwinery\npoint_price_ratio\n\n\n\n\n0\nUS\nThis tremendous 100% varietal wine hails from ...\nMartha's Vineyard\n96\n235.0\nCalifornia\nNapa Valley\nNapa\nCabernet Sauvignon\nHeitz\n0.408511\n\n\n1\nSpain\nRipe aromas of fig, blackberry and cassis are ...\nCarodorum Selección Especial Reserva\n96\n110.0\nNorthern Spain\nToro\nNaN\nTinta de Toro\nBodega Carmen Rodríguez\n0.872727\n\n\n2\nUS\nMac Watson honors the memory of a wine once ma...\nSpecial Selected Late Harvest\n96\n90.0\nCalifornia\nKnights Valley\nSonoma\nSauvignon Blanc\nMacauley\n1.066667\n\n\n3\nUS\nThis spent 20 months in 30% new French oak, an...\nReserve\n96\n65.0\nOregon\nWillamette Valley\nWillamette Valley\nPinot Noir\nPonzi\n1.476923\n\n\n4\nFrance\nThis is the top wine from La Bégude, named aft...\nLa Brûlade\n95\n66.0\nProvence\nBandol\nNaN\nProvence red blend\nDomaine de la Bégude\n1.439394\n\n\n\n\n\n\n\n\n# Filling in missing values\n# For the missing values in price, assign the mean price to it.\ndf_filled_in_price = df.loc[df['price'].isna(), 'price'] = df['price'].mean()\n\n\n\nString Methods\n\nPandas has a number of methods that allow you to work with string types\n\nFiltering\nManipulating\nConverting\nSubsetting\n\n\nExample: Find all entries containing the word aroma\n\ndf['description'].str.contains(\"aroma\").head()\n\n0    False\n1     True\n2    False\n3     True\n4    False\nName: description, dtype: bool\n\n\nreturns a description boolean series if the word aroma is in the description\n\n# Finding the entries in the dataframe\ndf.loc[df['description'].str.contains(\"aroma\")].head()\n\n\n\n\n\n\n\n\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\nvariety\nwinery\npoint_price_ratio\n\n\n\n\n1\nSpain\nRipe aromas of fig, blackberry and cassis are ...\nCarodorum Selección Especial Reserva\n96\n110.0\nNorthern Spain\nToro\nNaN\nTinta de Toro\nBodega Carmen Rodríguez\n0.872727\n\n\n3\nUS\nThis spent 20 months in 30% new French oak, an...\nReserve\n96\n65.0\nOregon\nWillamette Valley\nWillamette Valley\nPinot Noir\nPonzi\n1.476923\n\n\n6\nSpain\nSlightly gritty black-fruit aromas include a s...\nSan Román\n95\n65.0\nNorthern Spain\nToro\nNaN\nTinta de Toro\nMaurodos\n1.461538\n\n\n7\nSpain\nLush cedary black-fruit aromas are luxe and of...\nCarodorum Único Crianza\n95\n110.0\nNorthern Spain\nToro\nNaN\nTinta de Toro\nBodega Carmen Rodríguez\n0.863636\n\n\n10\nItaly\nElegance, complexity and structure come togeth...\nRonco della Chiesa\n95\n80.0\nNortheastern Italy\nCollio\nNaN\nFriulano\nBorgo del Tiglio\n1.187500\n\n\n\n\n\n\n\n\n# Get the first 10 characters for each of the \ndf['description'].str[:10].head()\n\n0    This treme\n1    Ripe aroma\n2    Mac Watson\n3    This spent\n4    This is th\nName: description, dtype: object\n\n\n\n# Join the first 10 chars and last 10 chars together \n# Wrap in parenthesis if chaining methods\n(df['description'].str[:10] + df['description'].str[-10:]).head()\n\n0    This treme2022–2030.\n1    Ripe aromaough 2023.\n2    Mac Watsonual sugar.\n3    This spentough 2032.\n4    This is thfrom 2020.\nName: description, dtype: object\n\n\n\n\nData Grouping\n\nWhat if we wanted to know the average points per country?\nWe need a tool to group categories together and perform operations.\n\n\nSplit-Apply-Combine\n\n\n# Break the Dataframe into a smaller Dataframe for each country label\n# Calculate the mean in each Dataframe \n\ndf.groupby('country')['points'].mean().head(3)\n\ncountry\nAlbania      88.000000\nArgentina    85.996093\nAustralia    87.892475\nName: points, dtype: float64\n\n\n\n# groupby example \n# Returns a series of mean points per country and province\ndf.groupby(['country', 'province'])['points'].mean().head()\n\ncountry    province        \nAlbania    Mirditë             88.000000\nArgentina  Mendoza Province    86.108182\n           Other               85.398200\nAustralia  Australia Other     84.813743\n           New South Wales     87.048780\nName: points, dtype: float64\n\n\n\ndf.groupby(['country', 'province'])['points', 'price'].mean().head()\n\n\n\n\n\n\n\n\n\npoints\nprice\n\n\ncountry\nprovince\n\n\n\n\n\n\nAlbania\nMirditë\n88.000000\n20.000000\n\n\nArgentina\nMendoza Province\n86.108182\n20.951441\n\n\nOther\n85.398200\n20.570362\n\n\nAustralia\nAustralia Other\n84.813743\n11.770819\n\n\nNew South Wales\n87.048780\n22.139280"
  },
  {
    "objectID": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#data-visualization",
    "href": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#data-visualization",
    "title": "Data Analysis with Pandas, Plotly and Seaborn - Part 1",
    "section": "Data Visualization",
    "text": "Data Visualization\nLets take the top 3 wine varieties and look at some visualization examples using plotly.\n\ndef select_by_top(_df, cat, n):\n    return ( _df[cat]\n            .value_counts()[:n]\n            .index\n            .tolist()\n           ) \n\n\nvis_df = df.loc[df['variety'].isin(select_by_top(df,'variety', 3)),]\nvis_df = vis_df.dropna()\n\n\nHistograms and Bars\nPreparing the data for counting\n\ncount_df = ( vis_df\n            .groupby('variety')\n            .size() # Count the amount of rows in the group\n            .rename('count') # Name the series count\n            .reset_index() # Set the group index as a column and create a new index\n            ) \n\n\ncount_df\n\n\n\n\n\n\n\n\nvariety\ncount\n\n\n\n\n0\nCabernet Sauvignon\n4889\n\n\n1\nChardonnay\n4892\n\n\n2\nPinot Noir\n7442\n\n\n\n\n\n\n\n\nSeabornPlotly\n\n\n\nax=sns.barplot(data=count_df, x='variety', y='count')\nax.set_title(\"Counting Variety\")\nplt.show()\n\n\n\n\n\n\n\npx.bar(count_df, x='variety', y='count', title='Counting Variety')\n\n\n                                                \n\n\n\n\n\nBreaking down by province\n\ncount_reg_df = ( vis_df\n            .groupby(['variety', 'province']) \n            .size() # Count the amount of rows in the group\n            .rename('count') # Name the series count\n            .reset_index() # Set the group index as a column and create a new index\n) \n\n\ncount_reg_df\n\n\n\n\n\n\n\n\nvariety\nprovince\ncount\n\n\n\n\n0\nCabernet Sauvignon\nCalifornia\n4040\n\n\n1\nCabernet Sauvignon\nNew York\n29\n\n\n2\nCabernet Sauvignon\nOregon\n42\n\n\n3\nCabernet Sauvignon\nWashington\n778\n\n\n4\nChardonnay\nCalifornia\n3992\n\n\n5\nChardonnay\nNew York\n275\n\n\n6\nChardonnay\nOregon\n239\n\n\n7\nChardonnay\nWashington\n386\n\n\n8\nPinot Noir\nCalifornia\n5424\n\n\n9\nPinot Noir\nNew York\n48\n\n\n10\nPinot Noir\nOregon\n1961\n\n\n11\nPinot Noir\nWashington\n9\n\n\n\n\n\n\n\n\nSeabornPlotly\n\n\n\nsns.barplot(data=count_reg_df, x='variety', y='count', hue='province');\n\n\n\n\n\n\n\npx.bar(count_reg_df, x='variety', y='count', color='province', title='Most Popular Varieties Count by Province' )\n\n\n                                                \n\n\n\n\n\n\n\nBox and Violin Plots\n\nSeabornPlotly\n\n\n\nax=sns.boxplot(data=vis_df, x='variety', y='price')\nax.set_title(\"Price Dist by Variety\")\nplt.show()\n\n\n\n\n\n\n\npx.box(vis_df, x='variety', y='price', title='Price Dist. By Variety')\n\n\n                                                \n\n\n\n\n\nHard to read? Zoom in!\n\nSeabornPlotly\n\n\n\nax=sns.violinplot(data=vis_df, x='variety', y='price')\nax.set_ylim(-3,300)\nax.set_title(\"Price Dist. by Variety\")\nplt.show()\n\n\n\n\n\n\n\npx.violin(vis_df, x='variety', y='price', title='Price Dist. By Variety', range_y=[-3,300])\n\n\n                                                \n\n\n\n\n\n\n\nECDF Plots\nVisualizing the empirical cumulative density function (ECDF)\n\nSeabornPlotly\n\n\n\nax = sns.ecdfplot(data=vis_df, x='price', hue='variety')\nax.set_title(\"ECDF Plot of Price by Variety\")\n\nText(0.5, 1.0, 'ECDF Plot of Price by Variety')\n\n\n\n\n\n\n\n\npx.ecdf(vis_df,x='price', color='variety', title='ECDF Plot of Price by Variety')\n\n\n                                                \n\n\n\n\n\n\n\nScatterplots\nPlotting Relationships\n\nSeabornPlotly\n\n\n\nax = sns.scatterplot(data=vis_df, x='price', y='points')\nax.set_title('Wine Price vs. Wine Rating')\nplt.show()\n\n\n\n\n\n\n\nfig = px.scatter(vis_df, x='price', y='points', title='Wine Price vs. Wine Rating')\nfig.show()\n\n\n                                                \n\n\n\n\n\nThis is looking a little cramped. Let’s try log scaling these plots.\n\nSeabornPlotly\n\n\n\nax = sns.scatterplot(data=vis_df, x='price', y='points')\nax.set_xscale('log')\nax.set_title(\"Wine Price vs. Wine Rating\")\nplt.show()\n\n\n\n\n\n\n\nfig = px.scatter(vis_df, x='price', y='points', title='Wine Price vs. Wine Rating', log_x=True)\nfig.show()\n\n\n                                                \n\n\n\n\n\nAdding Color\n\nSeabornPlotly\n\n\n\n#Adding Color\nax = sns.scatterplot(data=vis_df, x='price', y='points', hue='variety')\nax.set_xscale('log')\nax.set_title(\"Wine Price vs. Wine Rating\")\nplt.show()\n\n\n\n\n\n\n\nfig = px.scatter(vis_df, x='price', y='points', color='variety',title='Wine Price vs. Wine Rating', log_x=True, opacity=0.25)\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\nTrends\nAdding a Trendline\n\nSeabornPlotly\n\n\n\nfig = px.scatter(vis_df,\n                 x='price',\n                 y='points',\n                 title='Wine Price vs. Wine Rating',\n                 log_x=True,\n                 opacity=0.2,\n                 trendline='lowess')\nfig.show()\n\n\n                                                \n\n\n\n\n\nax = sns.regplot(data=vis_df, x='price', y='points', lowess=True, scatter_kws={'alpha': 0.1}, line_kws={'color': 'green'})\nax.set_xscale('log')\nax.set_title(\"Wine Price vs. Wine Rating\")\nplt.show()\n\n\n\n\n\n\n\nLooks good! But how do these trends change by variety? What about province?\n\n\nFaceting\nFaceting allow for visualizing by a group.\nWe can combine everything we’ve learned thus far into a a single figure.\n\nSeabornPlotly\n\n\n\ng = ( sns.lmplot(\n    data=vis_df, x='price', y='points', col='variety',\n     row='province', lowess=True, line_kws={'color':'green'},\n      scatter_kws={'alpha': 0.2}, facet_kws={'subplot_kws': {'xscale': 'log'}, 'margin_titles': True})\n    )\ng.set_titles(row_template = '{row_name}', col_template = '{col_name}')\nplt.show()\n\n\n\n\n\n\n\nfig = px.scatter(vis_df,\n                 x='price',\n                 y='points',\n                 hover_data=['winery'],\n                 facet_col='variety',\n                 title='Wine Price vs. Wine Rating by Variety',\n                 log_x=True,\n                 opacity=0.2,\n                 trendline='lowess',\n                 facet_row='province',)\nfig.for_each_annotation(lambda a: a.update(text=a.text.replace(\"province=\", \"\")))# Cleaning up the row facets \nfig.show()"
  },
  {
    "objectID": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#seaborn-vs.-plotly-pro-cons",
    "href": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#seaborn-vs.-plotly-pro-cons",
    "title": "Data Analysis with Pandas, Plotly and Seaborn - Part 1",
    "section": "Seaborn Vs. Plotly Pro / Cons",
    "text": "Seaborn Vs. Plotly Pro / Cons\nPlotly Pros:\n- Interactivity right out of the box. No assembly required!\n- Great for dashboarding.\n- Plotly express makes taking in pandas dataframes work well out of the box. \nPlotly Cons:\n- Newer and less mature than seaborn/matplotlib. May be harder to problem solve / debug. \nSeaborn Pros:\n- Matplotlib backend is more performant when plotting a lot of data.\n- Matplotlib backend is very mature and it's fairly easy to google your way into the plot you want.\nSeaborn Cons:\n- Interactivity is more challenging to implement.\n- API is complex and takes time to master when making customized plots."
  },
  {
    "objectID": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#tips",
    "href": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#tips",
    "title": "Data Analysis with Pandas, Plotly and Seaborn - Part 1",
    "section": "Tips",
    "text": "Tips\n\nKeep the number of intermediate DataFrames to a minimum\n\nHaving too many variables can confuse the reader and analyst. (Did I perform this operation on df85 or df86?)\nMethod chaining can help reduce the number of intermediate variables.\n\nCheck your work!\n\nIts very possible to screw up your data without realizing you have.\nBefore you show your conclusions to stakeholders, perform some reasonable data checks.\n\nPick a data vis tool and stick to it.\n\nEach takes a bit of time to master, and can be confusing to try and learn multiple data vis tools at once.\n\nThese tools have a lot of functionality and are complicated. Don’t be afraid to google if you can’t remember how to do something."
  },
  {
    "objectID": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#resources",
    "href": "posts/post-with-code/intro_to_pandas_mpl/intro_pandas_matplotlib.html#resources",
    "title": "Data Analysis with Pandas, Plotly and Seaborn - Part 1",
    "section": "Resources",
    "text": "Resources\n\nSeaborn API: https://seaborn.pydata.org/api.html\nGetting Started with Matplotlib: https://pythonprogramming.net/matplotlib-python-3-basics-tutorial/\nPandas user guide https://pandas.pydata.org/docs/user_guide/index.html"
  },
  {
    "objectID": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html",
    "href": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html",
    "title": "Bootstrapping Confidence",
    "section": "",
    "text": "Bootstrapping a non-parametric method method of sampling from a sample to help estimate a population parameter.\nOnce we generate many bootstrapped samples, we can apply a calculation to create an estimate for each, which the distribution can be analyzed.\nThis is a great alternative when independant samplings are unavailable."
  },
  {
    "objectID": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html#intro-to-bootstrapping",
    "href": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html#intro-to-bootstrapping",
    "title": "Bootstrapping Confidence",
    "section": "",
    "text": "Bootstrapping a non-parametric method method of sampling from a sample to help estimate a population parameter.\nOnce we generate many bootstrapped samples, we can apply a calculation to create an estimate for each, which the distribution can be analyzed.\nThis is a great alternative when independant samplings are unavailable."
  },
  {
    "objectID": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html#modeling-procedure",
    "href": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html#modeling-procedure",
    "title": "Bootstrapping Confidence",
    "section": "Modeling Procedure",
    "text": "Modeling Procedure\nFor each region:\n\nCreate a train and test dataset\nSplit the data into a training set and validation set at a ratio of 75:25.\nTrain the model and make predictions for the validation set.\nSave the predictions and true values for the validation set.\nPrint the average volume of predicted reserves and model RMSE.\nAnalyze the results."
  },
  {
    "objectID": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html#modeling-bootstrapping",
    "href": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html#modeling-bootstrapping",
    "title": "Bootstrapping Confidence",
    "section": "Modeling + Bootstrapping",
    "text": "Modeling + Bootstrapping\n\n\nImport Code\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.utils import resample\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set()\n\nstate = np.random.RandomState(42)\n\n\n\n\nData Import Code\nfilenames = [\"geo_data_\" + str(n) + \".csv\" for n in range(3)]\nregions = [\"g1\", \"g2\", \"g3\"]\nregion_dfs = {\n    region: pd.read_csv(Path(\"data\") / fn) for fn, region in zip(filenames, regions)\n}\n\nregion_dfs.get('g1').head().style\n\n\n\n\n\n\n\n \nid\nf0\nf1\nf2\nproduct\n\n\n\n\n0\ntxEyH\n0.705745\n-0.497823\n1.221170\n105.280062\n\n\n1\n2acmU\n1.334711\n-0.340164\n4.365080\n73.037750\n\n\n2\n409Wp\n1.022732\n0.151990\n1.419926\n85.265647\n\n\n3\niJLyR\n-0.032172\n0.139033\n2.978566\n168.620776\n\n\n4\nXdl7t\n1.988431\n0.155413\n4.751769\n154.036647\n\n\n\nExample of the data used to model and bootstrap\n\n\n\n\nBootstrap Function Code\nprice_per_1k_b = 4500\ncost_of_well = 500_000\nbudget_per_200_wells = cost_of_well * 200\n\n\ndef bootstrap_profit(outputs, n, top, profits_data):\n    return (\n        outputs.sample(n, random_state=state, replace=True)\n        .apply(lambda x: x.sort_values().tail(top).index)\n        .applymap(lambda x: profits_data.at[x])\n        .sum()\n    )\n\n\ndef bootstrap_top_sum(outputs, iters=10000, n=500, top=200, profits_data=None):\n    assert iters &lt; 1_000_000  # Don't crash your machine\n    return (\n        bootstrap_profit(outputs, n=n, top=top, profits_data=profits_data)\n        for iter in range(iters)\n    )\n\n\n\n\nModel Training Code\nmetrics_df_list = []\nfor region, oil_data in region_dfs.items():\n    X_train, X_val, y_train, y_val = train_test_split(\n        oil_data.drop([\"id\", \"product\"], axis=1),\n        oil_data[\"product\"],\n        random_state=state,\n    )\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    mean_model = [np.mean(y_train)] * y_val.shape[0]\n    preds = pd.Series(\n        model.predict(X_val), index=X_val.index, name=\"linear_model_choice\"\n    )\n    mean_preds = preds.mean()\n    rmse_linear = mean_squared_error(y_val, preds) ** (0.5)\n    rmse_mean_baseline = mean_squared_error(y_val, mean_model) ** (0.5)\n    well_rev = mean_preds * price_per_1k_b\n\n    print(\n        f\"\"\"{region}:\n        Linear Model RMSE of {rmse_linear:.2f}\n        Mean Model RMSE of {rmse_mean_baseline:.2f}\n        \"\"\"\n    )\n\n    [print(f\"Feature f{i} coefficient is: {model.coef_[i]:.2f}\") for i in range(3)]\n    print(\"\")\n\n    shuffled_index = np.random.choice(y_val.index.array, y_val.shape[0], replace=False)\n    shuffled_targets = y_val.copy().rename(\"random_choice\")\n    shuffled_targets.index = shuffled_index\n    preds_target_df = pd.concat(\n        [y_val.rename(\"perfect_choice\"), shuffled_targets, preds], axis=1\n    )\n\n    profits_data = y_val.mul(price_per_1k_b).subtract(cost_of_well)\n\n    metrics_df = pd.concat(\n        bootstrap_top_sum(outputs=preds_target_df, profits_data=profits_data)\n    )\n    metrics_df.index.name = \"model_type\"\n    metrics_df_list.append(\n        metrics_df.rename(\"profit\").reset_index().assign(region=region)\n    )\n\nprofit_region_df = pd.concat(metrics_df_list)\n\n\ng1:\n        Linear Model RMSE of 37.76\n        Mean Model RMSE of 44.28\n        \nFeature f0 coefficient is: 3.83\nFeature f1 coefficient is: -14.26\nFeature f2 coefficient is: 6.59\n\ng2:\n        Linear Model RMSE of 0.90\n        Mean Model RMSE of 46.00\n        \nFeature f0 coefficient is: -0.14\nFeature f1 coefficient is: -0.02\nFeature f2 coefficient is: 26.95\n\ng3:\n        Linear Model RMSE of 40.18\n        Mean Model RMSE of 44.91\n        \nFeature f0 coefficient is: -0.07\nFeature f1 coefficient is: -0.08\nFeature f2 coefficient is: 5.73\n\n\n\nRunning a linear model for each region indicates a few things:\n\nOur linear model is very effective in a single region: g2. Regions g1 and g3 have much smaller benefit from using a linear model over a simple mean estimate (consider this a reasonable baseline). If we considered these other regions potentially very profitable, we may want to try other models and determine if there’s an effective way of predicting the target.\nRegion g2 targets can be predicted very effectively with a linear model, specifically using the f2 feature."
  },
  {
    "objectID": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html#results",
    "href": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html#results",
    "title": "Bootstrapping Confidence",
    "section": "Results",
    "text": "Results\nBased on the RMSE values for each region shown above, It’s clear that a linear model is much less error prone in the g2 region. Given this information, the model will likely have a better chance at accurately predicting profitable wells.\nThe question now is: Can we use this information to estimate the profit and loss of future projects for each region? This is where the bootstrap comes in to simulate sampling from the population of wells for each region.\nIn order to calculate profit, we had generated bootstrapped samples, picked the top choices predicted from our model, then used and converted the true volumes to profit in millions, subtracting the cost to develop.\n\n\nTable Code\nprofit_region_df.groupby([\"model_type\", \"region\"]).describe()[\"profit\"].drop(\n    columns=[\"count\"]\n).divide(1e6).style.background_gradient(axis=0).format(\"{:.2f}\").set_caption(\n    \"Bootstrapped Profit Summary Statistics (USD Millions)\"\n)\n\n\n\n\n\nBootstrapped Profit Summary Statistics (USD Millions)\n\n\n \n \nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nmodel_type\nregion\n \n \n \n \n \n \n \n\n\n\n\nlinear_model_choice\ng1\n4.08\n2.60\n-5.47\n2.28\n4.10\n5.84\n13.42\n\n\ng2\n4.49\n2.06\n-3.17\n3.11\n4.48\n5.85\n11.93\n\n\ng3\n3.64\n2.68\n-8.81\n1.81\n3.68\n5.46\n13.73\n\n\nperfect_choice\ng1\n23.76\n1.99\n16.68\n22.40\n23.73\n25.13\n30.97\n\n\ng2\n4.51\n2.06\n-3.16\n3.13\n4.49\n5.86\n11.93\n\n\ng3\n26.72\n2.04\n18.58\n25.36\n26.72\n28.11\n33.32\n\n\nrandom_choice\ng1\n-17.05\n2.83\n-28.09\n-18.96\n-17.07\n-15.11\n-5.43\n\n\ng2\n-37.93\n2.97\n-48.10\n-39.93\n-37.87\n-35.97\n-27.67\n\n\ng3\n-14.09\n2.86\n-25.97\n-15.98\n-14.10\n-12.17\n-4.42\n\n\n\n\n\n\n\nPlotting Code\ndef plot_mil_formatter(x, pos):\n    return str(round(x / 1e6, 1)) + \" million\"\n\n\ng = sns.catplot(\n    data=profit_region_df,\n    x=\"model_type\",\n    y=\"profit\",\n    col=\"region\",\n    kind=\"box\",\n    order=[\"perfect_choice\", \"linear_model_choice\", \"random_choice\"],\n)\ng.axes[0][0].yaxis.set_major_formatter(plot_mil_formatter)\ng.axes[0][0].set_xlabel(\"\")\ng.axes[0][2].set_xlabel(\"\")\n\nfor ax in g.axes[0]:\n    ax.axhline(0, ls=\"--\", linewidth=2.1, color=\"black\")\nplt.show()\n\n\n\n\n\nBootstrapped profits by estimating the impact of the linear model making choices for each region\n\n\n\n\nThe plot above indicates that region g1 is very profitable if we were to choose the right locations for our wells, however, our model isn’t strong enough to predict us anywhere close to perfect choices for the region.\nHowever, if we were to want to minimize our risk, and our linear model was our best model, g2 would yeild us the lowest risk of losing money."
  },
  {
    "objectID": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html#conclusion",
    "href": "posts/post-with-code/bootstrapping/regression-with-bootstrapping.html#conclusion",
    "title": "Bootstrapping Confidence",
    "section": "Conclusion",
    "text": "Conclusion\nThe bootstrap provided a way to estimate the population distribution of profit using two different methods, and given the massive difference in profit distributions between the two model approaches in each region, it illustrates the effectiveness of our linear model to choose wells that will yield the most oil.\nWe used a random choice baseline to help quantify the effectiveness of a model over a “lack of model”.\nI would advise development to take place within the G2 region to maximize profit and minimize risk using the top choices provided by the linear model that was trained."
  },
  {
    "objectID": "posts/post-with-code/sql_basics/2020-06-30-sql-fun.html",
    "href": "posts/post-with-code/sql_basics/2020-06-30-sql-fun.html",
    "title": "SQL Practice",
    "section": "",
    "text": "Let’s solve a mystery hidden in an sql database.\nThe instructions and database was created by Joon Park and Cathy He while fellows at the Knight Lab at Northwesten University.\nHere’s a step by step solution, though if you’d like to try it yourself first, check out their interactive site.\nHere’s the description from the knight lab:\n\n\nA crime has taken place and the detective needs your help. The detective gave you the crime scene report, but you somehow lost it. You vaguely remember that the crime was a murder that occurred sometime on Jan.15, 2018 and that it took place in SQL City. Start by retrieving the corresponding crime scene report from the police department’s database.\n\n\nThe setup\nWe’ll use pandas to make the queries look nice and sqlite3 to make the connection.\nI’m also adding a function pq to quickly grab the results of a query based on the value of the query variable.\n\nimport pandas as pd\nimport sqlite3\nfrom sqlite3 import Error\nfrom pathlib import Path\n\n\ndef create_connection(path):\n\n    connection = None\n\n    try:\n\n        connection = sqlite3.connect(path)\n\n        print(\"Connection to SQLite DB successful\")\n\n    except Error as e:\n\n        print(f\"The error '{e}' occurred\")\n\n    return connection\n\n\ndb = Path(\"data/sql-murder-mystery.db\")\n\nconn = create_connection(db)\n\nConnection to SQLite DB successful\n\n\nTaking a look at all the tables by selecting name filtering by table.\n\ndef pq(query, conn=conn):\n    \"\"\"Returns a sql query as a pandas dataframe\"\"\"\n    return pd.read_sql(query, conn)\n\n\nquery = \"\"\"SELECT name \n  FROM sqlite_master\n where type = 'table'\"\"\"\npq(query)\n\n\n\n\n\n\n\n\nname\n\n\n\n\n0\ncrime_scene_report\n\n\n1\ndrivers_license\n\n\n2\nperson\n\n\n3\nfacebook_event_checkin\n\n\n4\ninterview\n\n\n5\nget_fit_now_member\n\n\n6\nget_fit_now_check_in\n\n\n7\nincome\n\n\n8\nsolution\n\n\n\n\n\n\n\nIn order to determine the structure of each table, we can run a query like this:\n\nquery = \"\"\"\n    SELECT sql \n    FROM sqlite_master\n    WHERE name = 'crime_scene_report'\n    \"\"\"\nprint(pq(query).iloc[0, 0])\n\nCREATE TABLE crime_scene_report (\n        date integer,\n        type text,\n        description text,\n        city text\n    )\n\n\nNow that we have the database loaded and the query print function, we’re ready to tackle this murder mystery.\n\n\nSolving the mystery\n\nFinding the witnesses\n\nquery = \"\"\"\nSELECT *\nFROM crime_scene_report\nLIMIT 5\"\"\"\npq(query)\n\n\n\n\n\n\n\n\ndate\ntype\ndescription\ncity\n\n\n\n\n0\n20180115\nrobbery\nA Man Dressed as Spider-Man Is on a Robbery Spree\nNYC\n\n\n1\n20180115\nmurder\nLife? Dont talk to me about life.\nAlbany\n\n\n2\n20180115\nmurder\nMama, I killed a man, put a gun against his he...\nReno\n\n\n3\n20180215\nmurder\nREDACTED REDACTED REDACTED\nSQL City\n\n\n4\n20180215\nmurder\nSomeone killed the guard! He took an arrow to ...\nSQL City\n\n\n\n\n\n\n\nLet’s start with what we know. According to the prompt, we know that this murder took place on Jan. 15 2018 in SQL City. This should be enough info to explore crime scene reports.\nWe care about the description, and want to filter by city, date, and type.\n\nquery = \"\"\"\nSELECT description \nFROM crime_scene_report \nWHERE city = 'SQL City' AND type = 'murder' AND date = 20180115\"\"\"\n\nprint(pq(query).iloc[0, 0])\n\nSecurity footage shows that there were 2 witnesses. The first witness lives at the last house on \"Northwestern Dr\". The second witness, named Annabel, lives somewhere on \"Franklin Ave\".\n\n\n\n\nFinding the interviews\nLocated witnesses presumably interviewed by detectives. Let’s take a quick look the interview table\n\nquery = \"\"\"\nSELECT * \nFROM interview\nLIMIT 5\n\"\"\"\n\npq(query)\n\n\n\n\n\n\n\n\nperson_id\ntranscript\n\n\n\n\n0\n28508\n‘I deny it!’ said the March Hare.\\n\n\n\n1\n63713\n\\n\n\n\n2\n86208\nway, and the whole party swam to the shore.\\n\n\n\n3\n35267\nlessons in here? Why, there’s hardly room for ...\n\n\n4\n33856\n\\n\n\n\n\n\n\n\n\nNot too useful (yet). We need to figure out how the person_id connects\n\nquery = \"\"\"SELECT sql \n  FROM sqlite_master\n where name = 'interview'\"\"\"\nprint(pq(query).iloc[0, 0])\n\nCREATE TABLE interview (\n        person_id integer,\n        transcript text,\n        FOREIGN KEY (person_id) REFERENCES person(id)\n    )\n\n\nAHA! So let’s connect person_id from the person table.\n\nquery = \"\"\"\nSELECT sql \nFROM sqlite_master\nWHERE name = 'person'\"\"\"\nprint(pq(query).iloc[0, 0])\n\nCREATE TABLE person (\n        id integer PRIMARY KEY,\n        name text,\n        license_id integer,\n        address_number integer,\n        address_street_name text,\n        ssn integer,\n        FOREIGN KEY (license_id) REFERENCES drivers_license(id)\n    )\n\n\nThe clues we have are: * Annabel, who lives on “Franklin Ave” * Someone else who lives on the last house on “Northwestern Dr”\nOnce we join both tables, a subquery will help capture the last house by finding the largest street number for Northwestern Dr.  We can use an fstring to inject the subquery, making it a bit more readable.\n\nsubq = \"\"\"\nSELECT MAX(address_number) \nFROM person \nWHERE address_street_name = 'Northwestern Dr'\n\"\"\"\n\nquery = f\"\"\"\nSELECT transcript\nFROM person AS p\nINNER JOIN interview AS i ON p.id = i.person_id\nWHERE (address_street_name = 'Franklin Ave' AND name LIKE 'Annabel%') OR (address_street_name = 'Northwestern Dr' AND address_number IN ({subq}));\n\"\"\"\n[print(f\"{v}\\n\") for v in pq(query)[\"transcript\"]]\n\nI heard a gunshot and then saw a man run out. He had a \"Get Fit Now Gym\" bag. The membership number on the bag started with \"48Z\". Only gold members have those bags. The man got into a car with a plate that included \"H42W\".\n\nI saw the murder happen, and I recognized the killer from my gym when I was working out last week on January the 9th.\n\n\n\n[None, None]\n\n\n\n\nExamining the crime scene\nLet’s extract the salient points:\n\nSuspect was at the gym on Jan 9th\nSuspect (same?) male and has Get Fit Now Bag, presumably a member with “48Z” as the starting membership id. Gold member bag. Car connected to suspect has plate number partial of “H42W”\n\nTaking a look at gym data:\n\nquery = \"\"\"SELECT sql \n  FROM sqlite_master\n where name = 'get_fit_now_check_in'\"\"\"\nprint(pq(query).iloc[0, 0])\n\nCREATE TABLE get_fit_now_check_in (\n        membership_id text,\n        check_in_date integer,\n        check_in_time integer,\n        check_out_time integer,\n        FOREIGN KEY (membership_id) REFERENCES get_fit_now_member(id)\n    )\n\n\n\nquery = \"\"\"SELECT sql \n  FROM sqlite_master\n where name = 'get_fit_now_member'\"\"\"\nprint(pq(query).iloc[0, 0])\n\nCREATE TABLE get_fit_now_member (\n        id text PRIMARY KEY,\n        person_id integer,\n        name text,\n        membership_start_date integer,\n        membership_status text,\n        FOREIGN KEY (person_id) REFERENCES person(id)\n    )\n\n\n\nquery = \"\"\"\nSELECT *\nFROM get_fit_now_check_in\nWHERE check_in_date = 20180109 AND membership_id LIKE '48Z%'\n\"\"\"\npq(query)\n\n\n\n\n\n\n\n\nmembership_id\ncheck_in_date\ncheck_in_time\ncheck_out_time\n\n\n\n\n0\n48Z7A\n20180109\n1600\n1730\n\n\n1\n48Z55\n20180109\n1530\n1700\n\n\n\n\n\n\n\nThis is useful. Let’s see if we can combine this with the info from the other witness to isolate a single suspect.\n\nsubq = \"\"\"\nSELECT *\nFROM get_fit_now_check_in\nWHERE check_in_date = 20180109 AND membership_id LIKE '48Z%'\n\"\"\"\n\nquery = f\"\"\"\nSELECT *\nFROM ({subq}) AS c\nINNER JOIN get_fit_now_member AS m ON m.id = c.membership_id\n\"\"\"\npq(query)\n\n\n\n\n\n\n\n\nmembership_id\ncheck_in_date\ncheck_in_time\ncheck_out_time\nid\nperson_id\nname\nmembership_start_date\nmembership_status\n\n\n\n\n0\n48Z7A\n20180109\n1600\n1730\n48Z7A\n28819\nJoe Germuska\n20160305\ngold\n\n\n1\n48Z55\n20180109\n1530\n1700\n48Z55\n67318\nJeremy Bowers\n20160101\ngold\n\n\n\n\n\n\n\nWe can take advantage of the ‘free’ filtration by performing an inner join on the membership id.\nUnfortunately we’re stuck between two people. Our last resort is the driver’s license.\n\n\nFinding the murderer\n\nquery = \"\"\"SELECT sql \n  FROM sqlite_master\n where name = 'drivers_license'\"\"\"\nprint(pq(query).iloc[0, 0])\n\nCREATE TABLE drivers_license (\n        id integer PRIMARY KEY,\n        age integer,\n        height integer,\n        eye_color text,\n        hair_color text,\n        gender text,\n        plate_number text,\n        car_make text,\n        car_model text\n    )\n\n\n\nquery = \"\"\"SELECT sql \n  FROM sqlite_master\n where name = 'person'\"\"\"\nprint(pq(query).iloc[0, 0])\n\nCREATE TABLE person (\n        id integer PRIMARY KEY,\n        name text,\n        license_id integer,\n        address_number integer,\n        address_street_name text,\n        ssn integer,\n        FOREIGN KEY (license_id) REFERENCES drivers_license(id)\n    )\n\n\nLots of interesting info in these tables, so lets select everything that has a pretty good match.\n\nquery = \"\"\"\nSELECT *\nFROM drivers_license\nWHERE plate_number LIKE '%H42W%' AND gender = 'male'\n\"\"\"\npq(query)\n\n\n\n\n\n\n\n\nid\nage\nheight\neye_color\nhair_color\ngender\nplate_number\ncar_make\ncar_model\n\n\n\n\n0\n423327\n30\n70\nbrown\nbrown\nmale\n0H42W2\nChevrolet\nSpark LS\n\n\n1\n664760\n21\n71\nblack\nblack\nmale\n4H42WR\nNissan\nAltima\n\n\n\n\n\n\n\nInteresting… lets see if it matches either of our gym members.\n\nsubq = \"\"\"\nSELECT *\nFROM drivers_license\nWHERE plate_number LIKE '%H42W%' AND gender = 'male'\n\"\"\"\n\nquery = f\"\"\"\nSELECT *\nFROM ({subq}) d\nINNER JOIN person p ON d.id = p.license_id\"\"\"\n\npq(query)\n\n\n\n\n\n\n\n\nid\nage\nheight\neye_color\nhair_color\ngender\nplate_number\ncar_make\ncar_model\nid\nname\nlicense_id\naddress_number\naddress_street_name\nssn\n\n\n\n\n0\n664760\n21\n71\nblack\nblack\nmale\n4H42WR\nNissan\nAltima\n51739\nTushar Chandra\n664760\n312\nPhi St\n137882671\n\n\n1\n423327\n30\n70\nbrown\nbrown\nmale\n0H42W2\nChevrolet\nSpark LS\n67318\nJeremy Bowers\n423327\n530\nWashington Pl, Apt 3A\n871539279\n\n\n\n\n\n\n\nAh! We found the suspect in the list, but let’s create a query that combines it all.\n\ng_subq = \"\"\"\nSELECT *\nFROM get_fit_now_check_in\nWHERE check_in_date = 20180109 AND membership_id LIKE '48Z%'\n\"\"\"\n\ng_query = f\"\"\"\nSELECT name\nFROM ({g_subq}) AS c\nINNER JOIN get_fit_now_member AS m ON m.id = c.membership_id\n\"\"\"\n\n\nsubq = \"\"\"\nSELECT *\nFROM drivers_license\nWHERE plate_number LIKE '%H42W%' AND gender = 'male'\n\"\"\"\n\nquery = f\"\"\"\nSELECT *\nFROM ({subq}) d\nINNER JOIN person p ON d.id = p.license_id\nWHERE name IN ({g_query})\n\"\"\"\npq(query)\n\n\n\n\n\n\n\n\nid\nage\nheight\neye_color\nhair_color\ngender\nplate_number\ncar_make\ncar_model\nid\nname\nlicense_id\naddress_number\naddress_street_name\nssn\n\n\n\n\n0\n423327\n30\n70\nbrown\nbrown\nmale\n0H42W2\nChevrolet\nSpark LS\n67318\nJeremy Bowers\n423327\n530\nWashington Pl, Apt 3A\n871539279\n\n\n\n\n\n\n\nPerhaps we found the murderer? Let’s see if there’s anything else we can find out about the suspect. Perhaps he was interviewed?\n\nquery = \"\"\"\nSELECT *\nFROM interview\nWHERE person_id = 67318\n\"\"\"\nprint(pq(query).iloc[0, 1])\n\nI was hired by a woman with a lot of money. I don't know her name but I know she's around 5'5\" (65\") or 5'7\" (67\"). She has red hair and she drives a Tesla Model S. I know that she attended the SQL Symphony Concert 3 times in December 2017.\n\n\n\nIt turns out we’re not done yet!\n\n\nFinding the real murderer (bonus round)\n\nBetween 65” AND 67”\nRed hair\nHigh income\nTesla Model S\nCheckin SQL Symphony Concert 3 times in December 2017\n\n\nquery = \"\"\"\nSELECT * \nFROM drivers_license \nWHERE height BETWEEN 65 AND 67 \n    AND car_make = 'Tesla' \n    AND gender = 'female' \n    AND hair_color = 'red'\n\n\"\"\"\n\npq(query)\n\n\n\n\n\n\n\n\nid\nage\nheight\neye_color\nhair_color\ngender\nplate_number\ncar_make\ncar_model\n\n\n\n\n0\n202298\n68\n66\ngreen\nred\nfemale\n500123\nTesla\nModel S\n\n\n1\n291182\n65\n66\nblue\nred\nfemale\n08CM64\nTesla\nModel S\n\n\n2\n918773\n48\n65\nblack\nred\nfemale\n917UU3\nTesla\nModel S\n\n\n\n\n\n\n\n\nquery = \"\"\"SELECT sql \n  FROM sqlite_master\n where name = 'facebook_event_checkin'\"\"\"\nprint(pq(query).iloc[0, 0])\n\nCREATE TABLE facebook_event_checkin (\n        person_id integer,\n        event_id integer,\n        event_name text,\n        date integer,\n        FOREIGN KEY (person_id) REFERENCES person(id)\n    )\n\n\n\nquery = \"\"\"SELECT sql \n  FROM sqlite_master\n where name = 'income'\"\"\"\nprint(pq(query).iloc[0, 0])\n\nCREATE TABLE income (\n        ssn integer PRIMARY KEY,\n        annual_income integer\n    )\n\n\nWe can create a single query and combine all\n\nquery = \"\"\"\nSELECT name, annual_income\nFROM facebook_event_checkin f\nINNER JOIN person p ON p.id = f.person_id \nINNER JOIN drivers_license d ON p.license_id = d.id\nINNER JOIN income i ON p.ssn = i.ssn\nWHERE date BETWEEN 20171200 AND 20171231\n    AND event_name = 'SQL Symphony Concert'\n    AND height BETWEEN 65 AND 67\n    AND hair_color = 'red'\n    AND gender = 'female'\n    AND car_make = 'Tesla'\n    AND car_model = 'Model S'\nGROUP BY person_id\nHAVING COUNT(person_id) &gt; 2\n\"\"\"\n\npq(query)\n\n\n\n\n\n\n\n\nname\nannual_income\n\n\n\n\n0\nMiranda Priestly\n310000\n\n\n\n\n\n\n\nLooks like a match!"
  },
  {
    "objectID": "posts/post-with-code/bikeshare/bikeshare.html",
    "href": "posts/post-with-code/bikeshare/bikeshare.html",
    "title": "Working With Polars",
    "section": "",
    "text": "import pandas as pd\nfrom glob import glob\nimport polars as pl\nimport requests\nimport plotly.express as px\nimport seaborn as sns\n\nsns.set()\n\n\nPandasPolars\n\n\n\ndf = (pd\n .concat([pd.read_csv(data) for data in glob(\"data/2*\")])\n .assign(started_at = lambda x: pd.to_datetime(x.started_at).dt.tz_localize(\"US/Central\", ambiguous=True)\n         , ended_at = lambda x: pd.to_datetime(x.ended_at).dt.tz_localize(\"US/Central\", ambiguous=True)\n         , is_member = lambda x: x.member_casual == 'member'\n         )\n           )\n\n\n\n\ndf = (\n    pl.scan_csv(\n        \"data/*\",\n        dtypes={\n            \"start_station_id\": str,\n            \"end_station_id\": str,\n            \"started_at\": pl.Datetime(time_zone=\"US/Central\"),\n            \"ended_at\": pl.Datetime(time_zone=\"US/Central\"),\n        },\n    )\n    .with_columns((pl.col(\"member_casual\") == \"member\").alias(\"is_member\"))\n    .collect()\n)\n\n\n\n\n\ndate_min, date_max = (\n    str(df.get_column(\"started_at\").min()).split(\" \")[0],\n    str(df.get_column(\"started_at\").max()).split(\" \")[0],\n)\n\n\nmystery_lat, mystery_long = 41.960000, -87.680000\n\nresp = requests.get(\n    f\"https://archive-api.open-meteo.com/v1/archive?latitude={mystery_lat}&longitude={mystery_long}&start_date={date_min}&end_date={date_max}&hourly=temperature_2m&temperature_unit=fahrenheit\"\n)\n\ntime_df = pl.DataFrame(\n    resp.json()[\"hourly\"], schema={\"time\": str, \"temperature_2m\": float}\n).with_columns(\n    pl.col(\"time\").str.to_datetime(time_zone=\"UTC\").dt.convert_time_zone(\"US/Central\")\n)\n\n\nPandasPolars\n\n\n\n(\n    df.to_pandas()\n    .set_index(\"started_at\")\n    .groupby(\"is_member\")\n    .ride_id.resample(\"30D\")\n    .count()\n    .unstack(0)\n    .plot()\n);\n\n\n\n\n\n\n\npx.line(\n    df.sort(\"started_at\")\n    .join_asof(time_df.sort(by=\"time\"), left_on=\"started_at\", right_on=\"time\")\n    .groupby_dynamic(\"started_at\", every=\"30d\", by=\"is_member\")\n    .agg(\n        [\n            pl.col(\"start_station_id\").count().alias(\"number_of_rides\"),\n            pl.col(\"temperature_2m\").mean().alias(\"avg temp\"),\n        ]\n    ),\n    x=\"started_at\",\n    y=\"number_of_rides\",\n    color=\"is_member\",\n    title=\"Bike Share Rides by membership\",\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A data science blog",
    "section": "",
    "text": "Bootstrapping Confidence\n\n\n\n\n\n\n\nanalysis\n\n\ndata viz\n\n\nstatistics\n\n\nmachine learning\n\n\npython\n\n\n\n\nAn practical example of bootstrapping in python.\n\n\n\n\n\n\nAug 16, 2023\n\n\nJordan Wilheim\n\n\n\n\n\n\n  \n\n\n\n\nSQL Practice\n\n\n\n\n\n\n\nsql\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nJordan Wilheim\n\n\n\n\n\n\n  \n\n\n\n\nData Analysis with Pandas, Plotly and Seaborn - Part 1\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ndata viz\n\n\n\n\nAn introduction to data analytics in python\n\n\n\n\n\n\nAug 1, 2023\n\n\nJordan Wilheim\n\n\n\n\n\n\n  \n\n\n\n\nWorking With Polars\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ndata viz\n\n\n\n\nComparing the speed of polars vs pandas\n\n\n\n\n\n\nAug 1, 2023\n\n\nJordan Wilheim\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nJordan Wilheim\n\n\n\n\n\n\nNo matching items"
  }
]